{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "accelerator": "GPU",
      "name": "Visualize modulation reconstructions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and run a LatentModulatedSIREN model pre-trained on CelebA-HQ-64\n",
        "This demo shows how to load the pretrained weights from a LatentModulatedSIREN model, from the paper [From data to functa: Your data point is a function and you can treat it like one]() (Dupont, Kim, Eslami, Rezende, Rosenbaum. 2022). It uses code from the official [JAX](https://github.com/google/jax) + [Haiku](https://github.com/deepmind/dm-haiku) implementation.\n",
        "\n",
        "\n",
        "It's recommended to use `Runtime->Change Runtime Type` to pick a GPU for speed."
      ],
      "metadata": {
        "id": "AL4xpQUrApkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "!pip install chex\n",
        "!pip install dm-haiku\n",
        "!pip install dill\n",
        "!pip install matplotlib\n",
        "!pip install optax\n",
        "!git clone https://github.com/deepmind/functa/\n",
        "import dill\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import os\n",
        "os.chdir('functa')\n",
        "import function_reps, pytree_conversions, helpers"
      ],
      "metadata": {
        "id": "tDwpv4Z3BcXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pretrained weights "
      ],
      "metadata": {
        "id": "GYOvEkUVFe9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwuzniVJAeHY"
      },
      "outputs": [],
      "source": [
        "# Load params of LatentModulatedSiren model\n",
        "mod_dim = 512  # choose one of 64, 128, 256, 512, 1024\n",
        "# Download pretrained weights\n",
        "os.environ['MOD_DIM'] = str(mod_dim)\n",
        "!wget https://storage.googleapis.com/dm-functa/celeba_params_${MOD_DIM}_latents.npz\n",
        "# Load pretrained weights\n",
        "path = f'celeba_params_{mod_dim}_latents.npz'\n",
        "with open(path, 'rb') as f:\n",
        "  ckpt = dill.load(f)\n",
        "params = ckpt['params']\n",
        "config = ckpt['config']\n",
        "assert config['model']['type'] == 'latent_modulated_siren'\n",
        "print(f'Loaded params for model with {mod_dim} latent dimensions.')\n",
        "# Create haiku transformed model that runs the forward pass.\n",
        "# Only keep configs needed for model construction from model config\n",
        "# `None` below ensures no error is given when already removed\n",
        "model_config = config['model'].copy()\n",
        "model_config.pop('type', None)\n",
        "model_config.pop('l2_weight', None)\n",
        "model_config.pop('noise_std', None)\n",
        "\n",
        "\n",
        "def model_net(coords):\n",
        "  hk_model = function_reps.LatentModulatedSiren(\n",
        "      out_channels=config['dataset']['num_channels'], **model_config)\n",
        "  return hk_model(coords)\n",
        "\n",
        "\n",
        "model = hk.without_apply_rng(hk.transform(model_net))\n",
        "\n",
        "# Define function that renders image from a single modulation\n",
        "weights, init_modulation = function_reps.partition_params(params)\n",
        "init_modulation, concat_idx, tree_def = pytree_conversions.pytree_to_array(\n",
        "    init_modulation)\n",
        "\n",
        "\n",
        "def render_image(modulation, coords):\n",
        "  modulation_tree = pytree_conversions.array_to_pytree(\n",
        "      modulation, concat_idx, tree_def)\n",
        "  modulated_params = function_reps.merge_params(weights, modulation_tree)\n",
        "  return model.apply(modulated_params, coords)\n",
        "\n",
        "\n",
        "# Use jit and vmap to render faster on a batch of modulations\n",
        "render_image = jax.jit(jax.vmap(render_image))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load modulation dataset and grab a batch of modulations"
      ],
      "metadata": {
        "id": "Z244qyv9FtCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load a batch of modulations.\n",
        "# Ensure that the modulation dataset has been downloaded to the correct dir.\n",
        "!wget https://storage.googleapis.com/dm-functa/celeba_modulations_${MOD_DIM}_latents.npz\n",
        "path = f'celeba_modulations_{mod_dim}_latents.npz'\n",
        "with open(path, 'rb') as f:\n",
        "  data = dill.load(f)\n",
        "  train_dict = data['train']\n",
        "  test_dict = data['test']\n",
        "bs = 9\n",
        "test_mods = test_dict['modulation'][:bs]\n",
        "assert test_mods.shape == (bs, mod_dim)"
      ],
      "metadata": {
        "id": "E6rICba5FsWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruct batch of modulations and visualize reconstructions"
      ],
      "metadata": {
        "id": "JxrfAxpAF5cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create coords and tile for vmapping\n",
        "coords = function_reps.get_coordinate_grid(config['dataset']['resolution'])\n",
        "coords = jnp.stack([coords for _ in range(bs)])  # (bs, H, W, 2)\n",
        "# Reconstruct test_mods\n",
        "rec = render_image(test_mods, coords)  # (bs, H, W, 3)\n",
        "\n",
        "# Plot reconstructions as a grid\n",
        "im_batch = helpers.image_grid_from_batch(rec)\n",
        "gridsize = int(np.floor(np.sqrt(bs)))\n",
        "figsize = 4\n",
        "fig, ax = plt.subplots(figsize=(gridsize * figsize, gridsize * figsize))\n",
        "ax.imshow(im_batch)\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N55fX7ncF4n2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
